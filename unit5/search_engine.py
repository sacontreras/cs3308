from abc import ABCimport numpy as npimport pandas as pdfrom common import dispfrom unit4.indexer_ds import InvertedIndexDSfrom common.sqlite_utils import SQLiteDBManagerclass SearchEngine(ABC):    def __init__(self, path_to_sqlite_inv_index="indexer_part2.db"):        self.idx_ds = InvertedIndexDS()        self.inv_idx_sqldbm = SQLiteDBManager(f"{path_to_sqlite_inv_index}")        super().__init__()                    def __build_simple_where_clause__(self, processed_tokenized_search_terms, conjunctive=False):        where_clause_cond_template = "Term=='{}'"        # first search term        where_clause = "WHERE " + where_clause_cond_template.format(processed_tokenized_search_terms[0])        # the rest of the search terms        if len(processed_tokenized_search_terms) > 1:            for st in processed_tokenized_search_terms[1:]:                where_clause += (" AND " if conjunctive else " OR ") + where_clause_cond_template.format(st)        # debug        # print(f"WHERE-clause: {where_clause}")        return where_clause                    def run(self):        b_quit = False                df_vocab = self.inv_idx_sqldbm.sql_query_to_df("""            SELECT                 *            FROM                 TermDictionary            ORDER BY                 TermId        """).set_index('TermId')                while not b_quit:            search_terms = input("Enter the search terms, each separated by a space (or just press ENTER to QUIT): ").strip()                        if len(search_terms) == 0:                b_quit = len(input("Are you sure you want to QUIT? (press ENTER again to confirm): ").strip()) == 0            else:                # the first thing we need to do is tokenize the search terms                processed_tokenized_search_terms = self.idx_ds.parse_tokenize(search_terms)                                print(f"\nraw search terms:\n\t'{search_terms}'")                print(f"\ntokenized/processed search terms:\n\t{processed_tokenized_search_terms}")                                b_terms_in_vocab = len(processed_tokenized_search_terms) > 0                                # if the tokenization vector is non-empty...                if b_terms_in_vocab:                                        # then the first thing we need to do here is ensure all terms are in the corpus vocabulary                                        # construct disjunctive WHERE-clause for query                    where_clause = self.__build_simple_where_clause__(processed_tokenized_search_terms) # defaults to disjunctive                                        # now do the actual search of the TermDictionary table using the above disjunction where-clause                    df_found_terms = self.inv_idx_sqldbm.sql_query_to_df(f"""                        SELECT                             TermId, Term, N, df_t, idf_t                         FROM                             TermDictionary                        {where_clause}                        ORDER BY                             TermId                    """).set_index('TermId')                                        # all search terms must be in the vocab... otherwise df_t is 0 and idf is infinity for the ones that aren't, which is obviously wrong                    if len(df_found_terms) != len(processed_tokenized_search_terms):                        # then some terms are not in the corpus vocab                        b_terms_in_vocab = False                                                        # RESULTS                if b_terms_in_vocab:                                        # vector for search terms: the term id gives us the index of the each component of the vocab that we need to set to 1                    v_st = np.zeros(len(df_vocab))                    for t_id, row in df_found_terms.iterrows():                        v_st[t_id-1] = 1                    v_st_norm = np.sqrt(v_st.dot(v_st))                    # print(f"v_st_norm: {v_st_norm}")                    # now normalize                    v_st = v_st/v_st_norm                                            disp("")                    disp(df_found_terms)                                        # now find all ids from Posting table of documents containing ALL search terms                    docs_ids_cont_all_st = None                    for st in processed_tokenized_search_terms:                        df_found_postings = self.inv_idx_sqldbm.sql_query_to_df(f"""                            SELECT                                 TermDictionary.Term, Posting.TermId, DocId, tf_t_d, Posting.idf_t, tfidf                            FROM                                 Posting                            JOIN                                 TermDictionary                                     ON                                        Posting.TermId = TermDictionary.TermId                            WHERE                                 TermDictionary.Term=='{st}'                            ORDER BY                                 DocId                        """)                                                doc_ids_cont_st = set(df_found_postings.DocId.unique())                        docs_ids_cont_all_st = doc_ids_cont_st if docs_ids_cont_all_st is None else docs_ids_cont_all_st.intersection(doc_ids_cont_st)                                                # stop early if conjunction of results is empty                        if len(docs_ids_cont_all_st) == 0:                            break                                        if len(docs_ids_cont_all_st) > 0:                        # print(f"docs_ids_cont_all_st: {docs_ids_cont_all_st}")                        disp("")                        disp("### SEARCH RESULTS: The following documents contain all of the above search terms (results ordered by cosine similarity)")                                                # prepare a new DataFrame in order to display the results                        df_results = pd.DataFrame(columns=['DocId', 'DocumentName', 'norm', 'cosine_similarity', 'tfidf'])                                                # finally, postings for all documents containing ALL search terms                        where_clause = f"WHERE Posting.DocId IN {tuple(docs_ids_cont_all_st)}" if len(docs_ids_cont_all_st) > 1 else f"WHERE Posting.DocId={list(docs_ids_cont_all_st)[0]}"                        df_found_postings = self.inv_idx_sqldbm.sql_query_to_df(f"""                            SELECT                                 Posting.DocId, DocumentDictionary.DocumentName, Posting.TermId, TermDictionary.Term, tf_t_d, Posting.idf_t, tfidf                            FROM                                 Posting                            JOIN                                 TermDictionary                                     ON                                        Posting.TermId = TermDictionary.TermId                                 JOIN                                DocumentDictionary                                    ON                                        DocumentDictionary.DocId = Posting.DocId                            {where_clause}                            ORDER BY                                 Posting.DocId, Posting.TermId                        """)                                                # debug                        # display(HTML(df_found_postings.set_index(['DocId', 'TermId']).to_markdown()))                                                                        # now compute cosine similary of each document in the results to v_st (the search-term vector)                        for doc_id in sorted(list(docs_ids_cont_all_st)):                            df_found_postings_for_doc_id = df_found_postings.query(f"DocId=={doc_id}")                                                        # display(HTML(df_found_postings_for_doc_id.set_index(['DocId', 'TermId']).to_markdown()))                                                        # build the vector corresponding to the doc                            v_doc = np.zeros(len(df_vocab))                            doc_name = None                            term_tfidf_vals = {}                            for _, row in df_found_postings_for_doc_id.iterrows():                                if doc_name is None:                                    doc_name = row['DocumentName']                                                                    # since we want to avoid reiterating as much as possible, we also fulfill the other required search results (namely, tfidf)                                if row['TermId'] in df_found_terms.index.values:                                    term_tfidf_vals[row['Term']] = row['tfidf']                                    # now set the term frequency of the component in the vector corresponding to this search term in the vocab                                v_doc[row['TermId']-1] = row['tf_t_d']                                                            v_doc_norm = np.sqrt(v_doc.dot(v_doc))                            # now normalize                            v_doc = v_doc/v_doc_norm                                                         # DEBUG: sanity check                            # for t_id, row in df_found_terms.iterrows():                            #     print(f"v_st[{t_id}]: {v_st[t_id]}, v_doc[{t_id}]: {v_doc[t_id]}") # neither should be 0                                                        cos_sim = v_st.dot(v_doc)                                                        data = [{                                'DocId': doc_id,                                 'DocumentName': doc_name,                                'norm': v_doc_norm,                                'cosine_similarity': cos_sim,                                'tfidf': term_tfidf_vals                            }]                            df_results = df_results.append(data, ignore_index=True, sort=False).sort_values(by='cosine_similarity', ascending=False)                                                                                                # FINALLY!! Display search results!                        df_results = df_results.set_index('DocId')                        # display(Markdown(df_results.head(20).to_markdown()))                        n_found = len(df_results)                        disp("")                        disp(f"{n_found} documents found" + (", displaying the first 20..." if n_found > 20 else ":"))                        disp("")                        disp(df_results.head(20))                                            else:                        disp("")                        disp("### SEARCH RESULTS: NO DOCUMENTS FOUND containing ALL of the above search terms")                                                        else:                    disp("")                    disp(f"some search terms from '{search_terms}' were not found in the (processed) corpus")                                print("\n\n")                        # we're done        disp("")        disp("")        disp("That's all, folks.  Thanks for playing!  BYE BYE.")